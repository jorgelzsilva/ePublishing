import zipfile
from lxml import etree
from colorama import Fore
import re

def check_toc_and_pagelist(epub_path):
    logs = []
    try:
        with zipfile.ZipFile(epub_path, 'r') as z:
            internal_files = [f.lower() for f in z.namelist()]
            internal_files_original = z.namelist()
            nav_file = next((f for f in z.namelist() if 'nav.xhtml' in f.lower()), None)
            ncx_file = next((f for f in z.namelist() if 'toc.ncx' in f.lower()), None)
            
            # Se n√£o achou nav.xhtml, tenta encontrar qualquer arquivo que pare√ßa ser um Nav EPUB 3
            if not nav_file:
                for f_name in z.namelist():
                    if f_name.lower().endswith(('.xhtml', '.html')):
                        with z.open(f_name) as f:
                            head = f.read(1024).decode('utf-8', errors='ignore').lower()
                            if '<nav' in head:
                                nav_file = f_name
                                break

            # Procura tamb√©m o sum√°rio visual (priorizando sumario.xhtml)
            names_to_check = ['sumario.xhtml', 'sum√°rio.xhtml']
            visual_toc_file = next((f for f in z.namelist() if any(name in f.lower() for name in names_to_check)), None)
            
            # Se n√£o achou sum√°rio, tenta outros nomes comuns
            if not visual_toc_file:
                visual_toc_file = next((f for f in z.namelist() if any(name in f.lower() for name in ['toc.xhtml', 'contents.xhtml'])), None)

            links_to_check = []
            pages_data = [] # Lista de dicion√°rios {"label": str, "href": str, "source": str}
            toc_type = ""

            # Prioriza Nav (EPUB 3)
            if nav_file:
                with z.open(nav_file) as f:
                    try:
                        tree = etree.parse(f)
                        links_to_check = tree.xpath('//*[local-name()="nav" and (contains(@*[local-name()="type"], "toc") or contains(@role, "toc"))]//*[local-name()="a"]')
                        pages_nodes = tree.xpath('//*[local-name()="nav" and (contains(@*[local-name()="type"], "page-list") or contains(@role, "pagelist"))]//*[local-name()="a"]')
                        for p in pages_nodes:
                            pages_data.append({
                                "label": "".join(p.xpath('.//text()')).strip(),
                                "href": p.get('href', ''),
                                "source": nav_file
                            })
                        toc_type = "Nav (EPUB 3)"
                        if links_to_check:
                            logs.append(f"üìö <strong>Sum√°rio T√©cnico:</strong> {toc_type}")
                            logs.append(f"   ‚îî‚îÄ Arquivo: <code>{nav_file}</code>")
                            print(f"    [      PASSOU      ] Sum√°rio Nav detectado.")
                    except:
                        pass

            # Se n√£o achou links no Nav, tenta NCX (EPUB 2)
            if not links_to_check and ncx_file:
                with z.open(ncx_file) as f:
                    tree = etree.parse(f)
                    links_to_check = tree.xpath('//*[local-name()="navPoint"]')
                    if not pages_data:
                        pages_nodes = tree.xpath('//*[local-name()="pageTarget"]')
                        for p in pages_nodes:
                            label = "".join(p.xpath('.//*[local-name()="navLabel"]//*[local-name()="text"]/text()')).strip()
                            href = p.xpath('.//*[local-name()="content"]/@src')
                            pages_data.append({
                                "label": label,
                                "href": href[0] if href else "",
                                "source": ncx_file
                            })
                    toc_type = "NCX (EPUB 2)"
                    logs.append(f"üìö <strong>Sum√°rio T√©cnico:</strong> {toc_type}")
                    logs.append(f"   ‚îî‚îÄ Arquivo: <code>{ncx_file}</code>")
                    print(f"    [      PASSOU      ] Sum√°rio NCX detectado.")
            
            # Se n√£o achou PageList no Nav ou NCX, tenta no Sum√°rio Visual (EPUB 3 structure)
            if not pages_data and visual_toc_file:
                with z.open(visual_toc_file) as f:
                    tree = etree.HTML(f.read())
                    pages_nodes = tree.xpath('//*[(@*[contains(local-name(), "type") and .="page-list"] or contains(@role, "pagelist"))]//*[local-name()="a"]')
                    for p in pages_nodes:
                        pages_data.append({
                            "label": "".join(p.xpath('.//text()')).strip(),
                            "href": p.get('href', ''),
                            "source": visual_toc_file
                        })

            # Fallback Brute-force: Escaneia todos os arquivos em busca de marcadores de p√°gina individuais
            if not pages_data:
                for f_name in z.namelist():
                    if f_name.lower().endswith(('.xhtml', '.html')):
                        with z.open(f_name) as f:
                            content = f.read().decode('utf-8', errors='ignore')
                            # Busca por marcadores e seus atributos
                            # Regex para pegar o marcador e tentar extrair id e label/text
                            marker_matches = re.finditer(r'<[^>]+(?:epub:type|role)=["\'](?:doc-)?pagebreak["\'][^>]*>', content, re.IGNORECASE)
                            for match in marker_matches:
                                tag_full = match.group(0)
                                pid = re.search(r'id=["\']([^"\']+)["\']', tag_full, re.IGNORECASE)
                                label = re.search(r'aria-label=["\']([^"\']+)["\']', tag_full, re.IGNORECASE) or re.search(r'title=["\']([^"\']+)["\']', tag_full, re.IGNORECASE)
                                
                                pages_data.append({
                                    "label": label.group(1) if label else (pid.group(1) if pid else "?"),
                                    "href": f"{f_name}#{pid.group(1)}" if pid else f_name,
                                    "source": "Scan Bruto"
                                })
                if pages_data:
                    logs.append(f"üìÑ <strong>PageList detectada via Scan de Marcadores:</strong> {len(pages_data)} encontrados.")

            # Valida√ß√£o real dos links com detalhamento
            broken = 0
            valid_links = []
            broken_links = []
            
            for link in links_to_check:
                href = link.get('href') or link.xpath('.//*[local-name()="content"]/@src')
                label = "".join(link.xpath('.//text()')).strip()[:100]  # Texto do link (max 100 chars)
                
                if href:
                    clean_href = href[0].split('#')[0] if isinstance(href, list) else href.split('#')[0]
                    original_href = href[0] if isinstance(href, list) else href
                    
                    # Verifica se o arquivo existe dentro do zip (busca flex√≠vel)
                    if any(clean_href.lower() in f for f in internal_files):
                        valid_links.append({"label": label, "href": original_href, "target": clean_href})
                    else:
                        broken += 1
                        broken_links.append({"label": label, "href": original_href, "target": clean_href})
            
            # Logs detalhados dos links do sum√°rio t√©cnico
            logs.append(f"")
            logs.append(f"üìñ <strong>Links do Sum√°rio T√©cnico:</strong> {len(links_to_check)} itens")
            
            if broken > 0:
                msg = f"{broken} links do sum√°rio est√£o quebrados ou √≥rf√£os."
                logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; font-weight:bold; color:#c0392b;'>[      FALHOU      ]</span> ‚îî‚îÄ {msg}")
                for bl in broken_links:
                    logs.append(f"      <span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> \"{bl['label']}\" ‚Üí <code>{bl['target']}</code> (N√ÉO ENCONTRADO)")
                print(f"{Fore.RED}    [      FALHOU      ] {msg}")
            else:
                msg = f"{len(links_to_check)} links validados com sucesso."
                logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; font-weight:bold; color:#27ae60;'>[      PASSOU      ]</span> ‚îî‚îÄ {msg}")
                for vl in valid_links:
                    logs.append(f"      <span style='font-family:monospace; color:#27ae60;'>[      PASSOU      ]</span> \"{vl['label']}\" ‚Üí <code>{vl['target']}</code>")
                print(f"{Fore.GREEN}    [      PASSOU      ] {msg}")
            
            # ===== VALIDA√á√ÉO DO SUM√ÅRIO VISUAL (HTML) =====
            if visual_toc_file:
                logs.append(f"")
                logs.append(f"üìë <strong>Sum√°rio Visual (HTML):</strong>")
                logs.append(f"   ‚îî‚îÄ Arquivo: <code>{visual_toc_file}</code>")
                print(f"    [      PASSOU      ] Sum√°rio visual detectado: {visual_toc_file}")
                
                with z.open(visual_toc_file) as f:
                    content_bytes = f.read()
                    tree = etree.HTML(content_bytes)
                    
                    # Busca todos os links do sum√°rio visual
                    visual_links = tree.xpath('//a[@href]')
                    
                    visual_valid = []
                    visual_broken = []
                    title_warnings = []
                    anchor_errors = []
                    label_content_errors = []
                    duplicate_links = {} # href -> count
                    
                    # Para detectar texto sem link, vamos coletar n√≥s de texto fora de <a>
                    body = tree.xpath('//body')[0] if tree.xpath('//body') else tree
                    
                    for a_tag in visual_links:
                        href = a_tag.get('href', '')
                        
                        # Ignora links externos
                        if href.startswith('http') or href.startswith('mailto:'):
                            continue
                        
                        # Texto dentro da tag <a>
                        text_inside_a = "".join(a_tag.xpath('.//text()')).strip()
                        
                        # Detec√ß√£o de duplicados
                        duplicate_links[href] = duplicate_links.get(href, 0) + 1
                        
                        # Verifica se o arquivo de destino existe
                        clean_href = href.split('#')[0]
                        anchor = href.split('#')[1] if '#' in href else None
                        
                        # Encontra o caminho correto do arquivo
                        toc_dir = "/".join(visual_toc_file.split('/')[:-1])
                        if toc_dir:
                            full_path = f"{toc_dir}/{clean_href}".replace('//', '/')
                        else:
                            full_path = clean_href
                        
                        file_exists = any(full_path.lower() in f.lower() for f in internal_files)
                        
                        if file_exists:
                            content_status = " ‚è≥" # Default status
                            
                            # Busca o arquivo real (mantendo camelcase se necess√°rio)
                            actual_file_in_zip = next((f for f in internal_files_original if full_path.lower() in f.lower()), None)
                            
                            if actual_file_in_zip:
                                with z.open(actual_file_in_zip) as tf:
                                    target_content_bytes = tf.read()
                                    target_tree = etree.HTML(target_content_bytes)
                                    target_text_all = " ".join(target_tree.xpath('//body//text()')).lower()
                                    
                                    # 1. Verifica se o texto do link existe no destino (ignora se for muito curto como "Cap 1")
                                    if len(text_inside_a) > 5:
                                        # Normaliza√ß√£o robusta: remove pontua√ß√£o, espa√ßos especiais e converte para min√∫sculo
                                        def norm(txt):
                                            t = re.sub(r'\s+', ' ', txt) # Normaliza espa√ßos
                                            return re.sub(r'[^\w\s]', '', t).lower().strip()

                                        def norm_extreme(txt):
                                            # Remove absolutamente tudo que n√£o for letra ou n√∫mero
                                            return re.sub(r'[^\w]', '', txt).lower()

                                        clean_label = norm(text_inside_a)
                                        clean_target = norm(target_text_all)
                                        
                                        # Busca exata (normalizada)
                                        found = clean_label in clean_target
                                        
                                        # Se n√£o achar, tenta remover prefixos como "Cap√≠tulo 1", "Parte 1", etc.
                                        if not found:
                                            # Remove prefixo: Palavra + N√∫mero + Pontuadores opcionais (suporta acentos)
                                            prefix_pattern = r'^(cap[i√≠]tulo|cap|parte|item|se√ß[a√£]o|secao|unidade|ap[e√™]ndice|apendice)\s+\d+[\s\.\-‚Äî‚Äì]*'
                                            shorter = re.sub(prefix_pattern, '', clean_label)
                                            if shorter != clean_label and shorter.strip():
                                                found = shorter.strip() in clean_target
                                            
                                            # Terceira tentativa: Normaliza√ß√£o extrema (sem espa√ßos)
                                            # Resolve problemas de palavras cortadas em m√∫ltiplos <span> (ex: <span>d</span><span>e</span>)
                                            if not found:
                                                found = norm_extreme(text_inside_a) in norm_extreme(target_text_all)

                                        if found:
                                            content_status = "<span style='font-family:monospace; color:#27ae60;'>[      PASSOU      ]</span> (Conte√∫do verificado) "
                                        else:
                                            content_status = "<span style='font-family:monospace; color:#e74c3c;'>[      FALHOU      ]</span> (‚ö†Ô∏è Texto n√£o encontrado!) "
                                            label_content_errors.append({
                                                "label": text_inside_a,
                                                "file": clean_href
                                            })
                                    else:
                                        content_status = "<small style='color:#bdc3c7'>(Texto curto, conte√∫do ignorado)</small> "

                                    # 2. Verifica se a √¢ncora existe no arquivo de destino
                                    if anchor:
                                        target_content = target_content_bytes.decode('utf-8', errors='ignore')
                                        if f'id="{anchor}"' not in target_content and f"id='{anchor}'" not in target_content:
                                            anchor_errors.append({"href": href, "anchor": anchor, "file": clean_href})
                            
                            visual_valid.append({
                                "label": text_inside_a[:100], 
                                "href": href, 
                                "target": clean_href,
                                "status": content_status
                            })
                        else:
                            visual_broken.append({"label": text_inside_a[:100], "href": href, "target": clean_href})
                    
                    # Detec√ß√£o de texto solto no sum√°rio (n√£o vinculado a <a>) usando XPath
                    unlinked_nodes = body.xpath('.//text()[not(ancestor::a)]')
                    unlinked_text_parts = [re.sub(r'\s+', ' ', str(n)).strip() for n in unlinked_nodes if str(n).strip()]
                    unlinked = " ".join(unlinked_text_parts)
                    unlinked = re.sub(r'[\d\s\.\-‚Äì‚Äî\‚Ä¢\|]+', ' ', unlinked).strip()
                    
                    # Log dos resultados do sum√°rio visual
                    logs.append(f"")
                    logs.append(f"üìñ <strong>Links do Sum√°rio Visual:</strong> {len(visual_valid) + len(visual_broken)} itens")
                    
                    if visual_broken:
                        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; font-weight:bold; color:#c0392b;'>[      FALHOU      ]</span> {len(visual_broken)} links quebrados")
                        for bl in visual_broken:
                            logs.append(f"      <span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> \"{bl['label']}\" ‚Üí <code>{bl['target']}</code> (N√ÉO ENCONTRADO)")
                        print(f"{Fore.RED}    [      FALHOU      ] Sum√°rio visual: {len(visual_broken)} links quebrados")
                    else:
                        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; font-weight:bold; color:#27ae60;'>[      PASSOU      ]</span> {len(visual_valid)} links validados")
                        print(f"{Fore.GREEN}    [      PASSOU      ] Sum√°rio visual: {len(visual_valid)} links validados")
                    
                    # Mostra todos os links do sum√°rio visual com status de verifica√ß√£o
                    for vl in visual_valid:
                        logs.append(f"      {vl['status']} \"{vl['label']}\" ‚Üí <code>{vl['target']}</code>")
                    
                    # Relat√≥rio de duplicados
                    dups = [h for h, c in duplicate_links.items() if c > 1]
                    if dups:
                        logs.append(f"")
                        logs.append(f"üîÑ <strong>Links Duplicados:</strong> {len(dups)} detectados")
                        for d in dups:
                            logs.append(f"   ‚îî‚îÄ <code>{d}</code> aparece {duplicate_links[d]} vezes")

                    # Inconsist√™ncia de Conte√∫do (Texto do link n√£o achado no destino)
                    if label_content_errors:
                        logs.append(f"")
                        logs.append(f"‚ùì <strong>Inconsist√™ncia de Conte√∫do:</strong>")
                        for lce in label_content_errors:
                            logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#f39c12;'>[      AVISO       ]</span> Texto <code>\"{lce['label']}\"</code> n√£o encontrado em <code>{lce['file']}</code>")
                        print(f"{Fore.YELLOW}    [      AVISO       ] {len(label_content_errors)} t√≠tulos n√£o encontrados no conte√∫do de destino")

                    # Texto sem link
                    if len(unlinked) > 10:
                        logs.append(f"")
                        logs.append(f"‚ö†Ô∏è <strong>Texto sem link detectado no Sum√°rio:</strong>")
                        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#f39c12;'>[      AVISO       ]</span> <em>\"{unlinked[:100]}...\"</em>")
                        print(f"{Fore.YELLOW}    [      AVISO       ] Texto sem link detectado no sum√°rio visual")

                    # Warnings sobre t√≠tulos parcialmente fora do <a>
                    if title_warnings:
                        logs.append(f"")
                        logs.append(f"‚ö†Ô∏è <strong>T√≠tulos parcialmente fora do &lt;a&gt;:</strong>")
                        for tw in title_warnings:
                            logs.append(f"   ‚îî‚îÄ \"{tw['title']}...\" tem texto fora: <code>{tw['outside']}</code>")
                        print(f"{Fore.YELLOW}    [!] {len(title_warnings)} t√≠tulos com texto fora do link")
                    
                    # Erros de √¢ncoras n√£o encontradas
                    if anchor_errors:
                        logs.append(f"")
                        logs.append(f"‚ùå <strong>√Çncoras n√£o encontradas:</strong>")
                        for ae in anchor_errors:
                            logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> <code>#{ae['anchor']}</code> n√£o existe em <code>{ae['file']}</code>")
                        print(f"{Fore.RED}    [      FALHOU      ] {len(anchor_errors)} √¢ncoras n√£o encontradas nos arquivos destino")
            
            # Valida√ß√£o Modular da PageList
            if pages_data:
                pl_ok, pl_logs = validate_pagelist_integrity(z, pages_data)
                logs.extend(pl_logs)
            else:
                logs.append("‚ÑπÔ∏è Nenhuma PageList encontrada (opcional para EPUB 3).")
                print(f"{Fore.YELLOW}    [      AVISO       ] Nenhuma PageList encontrada.")

            return True, logs
    except Exception as e:
        msg = f"Erro estrutural cr√≠tico: {e}"
        logs.append(f"<span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> {msg}")
        print(f"{Fore.RED}    [      FALHOU      ] {msg}")
        return False, logs

def validate_pagelist_integrity(z, pages_data):
    """
    Fun√ß√£o dedicada para validar a integridade da lista de p√°ginas.
    Verifica sequ√™ncia num√©rica e exist√™ncia de IDs.
    """
    logs = []
    if not pages_data:
        return True, []

    logs.append(f"üìÑ <strong>Validando PageList:</strong> {len(pages_data)} itens detectados.")
    
    # 1. Valida√ß√£o de Sequ√™ncia Num√©rica
    sequence_errors = []
    last_val = 0
    duplicate_pages = {}
    
    for i, p in enumerate(pages_data):
        label = p.get('label', '')
        # Extrai o primeiro n√∫mero encontrado no label
        nums = re.findall(r'\d+', label)
        if nums:
            current_val = int(nums[0])
            if last_val != 0 and current_val != last_val + 1:
                # Se for o mesmo n√∫mero, √© duplicado
                if current_val == last_val:
                    duplicate_pages[current_val] = duplicate_pages.get(current_val, 0) + 1
                else:
                    sequence_errors.append(f"Salto: {last_val} ‚Üí {current_val} (Item {i+1})")
            last_val = current_val

    if sequence_errors:
        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#f39c12;'>[      AVISO       ]</span> ‚ö†Ô∏è Sequ√™ncia num√©rica com saltos: {', '.join(sequence_errors)}")
    if duplicate_pages:
        dup_list = [str(k) for k in duplicate_pages.keys()]
        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> P√°ginas repetidas detectadas: {', '.join(dup_list)}")

    # 2. Valida√ß√£o de Exist√™ncia de IDs (√Çncoras)
    broken_ids = []
    internal_files = [f.lower() for f in z.namelist()]
    
    # Cache de arquivos j√° verificados para performance
    content_cache = {}

    for p in pages_data:
        href = p.get('href', '')
        if not href: continue
        
        target_file = href.split('#')[0]
        anchor = href.split('#')[1] if '#' in href else None
        
        # Resolve caminho relativo (se o source for em subpasta)
        source_file = p.get('source', '')
        if source_file and '/' in source_file and not target_file.startswith(('/', '..')):
            toc_dir = "/".join(source_file.split('/')[:-1])
            full_path = f"{toc_dir}/{target_file}".replace('//', '/').lower()
        else:
            full_path = target_file.lower()

        # Verifica se o arquivo existe
        actual_file = next((f for f in z.namelist() if full_path in f.lower()), None)
        if not actual_file:
            broken_ids.append(f"Arquivo n√£o localizado: <code>{target_file}</code>")
            continue

        # Verifica ID se houver
        if anchor:
            if actual_file not in content_cache:
                with z.open(actual_file) as f:
                    content_cache[actual_file] = f.read().decode('utf-8', errors='ignore')
            
            content = content_cache[actual_file]
            if f'id="{anchor}"' not in content and f"id='{anchor}'" not in content:
                broken_ids.append(f"ID <code>#{anchor}</code> n√£o encontrado em <code>{actual_file}</code>")

    if broken_ids:
        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> Erros de destino ({len(broken_ids)}):")
        for error in broken_ids:
            logs.append(f"      <span style='font-family:monospace; color:#c0392b;'>[      FALHOU      ]</span> {error}")
    else:
        logs.append(f"   ‚îî‚îÄ <span style='font-family:monospace; color:#27ae60;'>[      PASSOU      ]</span> Todos os destinos (arquivos e IDs) foram validados.")

    # Status final da PageList
    overall_ok = len(broken_ids) == 0 and not duplicate_pages
    return overall_ok, logs

# L√≥gica para validar se o ID do link existe no destino
def validate_anchor(zip_ref, href):
    if '#' not in href: return True # Link para o arquivo todo
    file_part, anchor = href.split('#')
    # Abre o arquivo de destino e procura o ID
    with zip_ref.open(file_part) as f:
        content = f.read().decode('utf-8')
        return f'id="{anchor}"' in content or f"id='{anchor}'" in content



def get_typesetting_credit(epub_path):
    """
    Procura por arquivos de cr√©ditos ou rosto e extrai quem fez a editora√ß√£o e/ou produ√ß√£o digital.
    """
    try:
        found_credits = []
        with zipfile.ZipFile(epub_path, 'r') as z:
            # Arquivos prov√°veis de conter cr√©ditos
            credit_files = [f for f in z.namelist() if any(name in f.lower() for name in ['credito', 'credit', 'rosto', 'copyright', 'copy'])]
            
            for file_path in credit_files:
                with z.open(file_path) as f:
                    content_bytes = f.read()
                    tree = etree.HTML(content_bytes)
                    
                    # Busca todos os par√°grafos
                    paragraphs = tree.xpath('//p')
                    for p in paragraphs:
                        # Pega todo o texto dentro do par√°grafo (incluindo spans internos)
                        full_text = "".join(p.xpath('.//text()')).strip()
                        
                        # Procura os termos Editora√ß√£o ou Produ√ß√£o digital
                        if any(term in full_text for term in ["Editora√ß√£o", "Produ√ß√£o digital", "Produ√ß√£o Digital"]):
                            if full_text not in found_credits:
                                found_credits.append(full_text)
            
        if found_credits:
            # Retorna os cr√©ditos encontrados separados por barra
            return " | ".join(found_credits)
            
        return "N√£o identificado"
    except Exception:
        return "Erro na extra√ß√£o"

def check_filenames(epub_path):
    """
    Verifica se os nomes de arquivos dentro do EPUB seguem as restri√ß√µes da plataforma:
    A-Z, a-z, 0-9, _, - (e o ponto para extens√µes e barra para diret√≥rios)
    """
    invalid_files = []
    # Regex para permitir apenas Alfanum√©ricos, Underscore, Dash, Ponto e Barra
    # Nota: A mensagem da plataforma n√£o citou ponto nem barra explicitamente, 
    # mas s√£o essenciais para caminhos e extens√µes de arquivos.
    pattern = re.compile(r'^[A-Za-z0-9_\-\./]+$')
    
    try:
        with zipfile.ZipFile(epub_path, 'r') as z:
            for file_info in z.infolist():
                if not pattern.match(file_info.filename):
                    invalid_files.append(file_info.filename)
        
        return invalid_files
    except Exception:
        return []